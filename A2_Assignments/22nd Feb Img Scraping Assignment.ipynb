{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048dac96-f00e-45a1-8869-cef79127377e",
   "metadata": {},
   "source": [
    "Go to this given URL and solve the following questions\n",
    "URL: https://www.youtube.com/@PW-Foundation/videos\n",
    "\n",
    "Q1. Write a python program to extract the video URL of the first five videos.\n",
    "\n",
    "Q2. Write a python program to extract the URL of the video thumbnails of the first five videos.\n",
    "\n",
    "Q3. Write a python program to extract the title of the first five videos.\n",
    "\n",
    "Q4. Write a python program to extract the number of views of the first five videos.\n",
    "\n",
    "Q5. Write a python program to extract the time of posting of video for the first five videos.\n",
    "\n",
    "Note: Save all the data scraped in the above questions in a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0541bb-66ad-4b1c-8a3a-33ec74ba051d",
   "metadata": {},
   "source": [
    "### Q1. Write a python program to extract the video URL of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c0a9ef-2c80-49ec-ae85-983486d72448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import logging\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae484f71-d145-4e6b-a578-f9411ed57ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'}\n",
    "link = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "\n",
    "response = requests.get(link, headers=headers) # The get() method sends a GET request to the specified url along with the headers to prevent website to block access.\n",
    "response_html = response.text # extracting the content from the response object\n",
    "\n",
    "soup = BeautifulSoup(response_html, \"html.parser\") # parsing the data through html.parser \n",
    "response.close() # closing the connection\n",
    "\n",
    "box = soup.find_all(\"script\") # videos ids are present in the javascript which are dynamic\n",
    "\n",
    "box_string = [i.get_text() for i in box] # extracting text from teh script tag\n",
    "\n",
    "ls = []\n",
    "for i in box_string:\n",
    "    if \"responseContext\" in i : # video's Id's are present under \"responseContext\" in javascript\n",
    "        ls.append(i.replace('\"', \"\").split(':')) # replaces the strings \" with nothing & then splits the string with ':', then it return nested list\n",
    "    \n",
    "video_urls = list()\n",
    "for i in ls: # looping nested list \n",
    "    for j in i:\n",
    "        if '/watch' in j: # actual location of video's ID\n",
    "            video_id = j.split(',')[0]\n",
    "            youtube_url = \"https://www.youtube.com{}\".format(video_id) # URL format\n",
    "            video_urls.append(youtube_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ed0faf0b-bbb0-41bc-abfc-07046c52adc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "api_key = \"AIzaSyA8JeiyqrZeff8BL77Jcvvuibax34PJVis\" # your youtube api key (can get through google deveoper console)\n",
    "\n",
    "soup = BeautifulSoup(urlopen('https://www.youtube.com/@PW-Foundation/videos'))\n",
    "channel_id = soup.find_all('meta', {'itemprop' : 'channelId'})[0].attrs['content'] # channelID from metadata\n",
    "\n",
    "video_url_string = 'https://www.youtube.com/watch?v='\n",
    "\n",
    "# count set to 10, api will fetch javascript for those videos throug API\n",
    "snippet_url = \"https://www.googleapis.com/youtube/v3/search?key={}&channelId={}&part=snippet,id&order=date&maxResults=10\".format(api_key, channel_id)\n",
    "\n",
    "url_d = urlopen(snippet_url) # open the connection through google api\n",
    "\n",
    "response_data = json.load(url_d) # converts the resonse into json\n",
    "\n",
    "url_d.close() # closes the connection\n",
    "\n",
    "video_ids = [item['id']['videoId'] for item in response_data['items']] # data in dictionaty format & fetch unique videoId of each video from the channel page.\n",
    "\n",
    "contents_data = []\n",
    "for video_id in video_ids :\n",
    "    # iterate api for each video_id\n",
    "    content_statistics_url = 'https://www.googleapis.com/youtube/v3/videos?part=snippet%2CcontentDetails%2Cstatistics&id={}&key={}'.format(video_id, api_key)\n",
    "    content_url_d = urlopen(content_statistics_url) # open the api connection\n",
    "    content_response_data = json.load(content_url_d) # jsonifying the data\n",
    "    contents_data.append(content_response_data) # appending response to list\n",
    "    content_url_d.close() # close the connection\n",
    "\n",
    "with open('Example.csv', 'w', newline = '', encoding='UTF8') as csvfile: # csv data object\n",
    "\n",
    "    fields = [\"S.No\",\"video_url\",\"thumbnail_url\",\"title\",\"no_of_views\", 'published_at'] # headers for the csv sheet\n",
    "    my_writer = csv.DictWriter(csvfile, fields) # writes the data through key, value pairs\n",
    "    my_writer.writeheader() # writing the header first\n",
    "\n",
    "    ls = list()\n",
    "    for index, content in enumerate(contents_data) : # fetch the data from the responses of each video_id\n",
    "        for item in content['items'] :\n",
    "            d = dict()\n",
    "            d['S.No'] = index + 1\n",
    "            d['video_url'] = \"{}{}\".format(video_url_string, item['id'])\n",
    "            d['thumbnail_url'] = item['snippet']['thumbnails']['default']['url']\n",
    "            d['title'] = item['snippet']['title']\n",
    "            d['no_of_views'] = item['statistics']['viewCount']\n",
    "            d['published_at'] = item['snippet']['publishedAt']\n",
    "            ls.append(d) # appending the dictionary as a single row\n",
    "\n",
    "    sorted(ls, key=lambda x:x['published_at']) # sorting the data in the list based on publishing time\n",
    "    \n",
    "    for row in ls :\n",
    "        my_writer.writerow(row) # writing the data to csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c33e6-8d29-462a-9948-ac0083e96b45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
