{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a21a92-bd5d-4b6f-9e3c-da1a78ba21fa",
   "metadata": {},
   "source": [
    "### Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n",
    "    \n",
    "        Missing values refer to the absence of data for one or more attributes in a dataset. This could happen due to various reasons such as human errors, technical errors, or data privacy concerns. It is essential to handle missing values because it can negatively impact the analysis of the dataset, leading to biased or inaccurate results.\n",
    "\n",
    "        Some algorithms that are not affected by missing values are:\n",
    "\n",
    "            Decision trees: Decision trees can handle missing values by assigning weights to each attribute based on its importance in predicting the target variable.\n",
    "\n",
    "            K-nearest neighbors: K-nearest neighbors algorithm can simply ignore the missing values and calculate the distance between instances based on the available attributes.\n",
    "\n",
    "            Naive Bayes: Naive Bayes assumes that the features are conditionally independent given the class, so missing values can be safely ignored.\n",
    "\n",
    "            Principal Component Analysis (PCA): PCA can handle missing values by estimating the missing values based on the correlations between attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157dee31-ac1e-417b-bd32-88a48f244923",
   "metadata": {},
   "source": [
    "### Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "    \n",
    "        Here are the few techniques used to handle missing data : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9ea0a031-31b9-4bd6-b1e6-6125392203a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before deletion\n",
      "      A     B\n",
      "0  1.0   6.0\n",
      "1  2.0   NaN\n",
      "2  3.0   8.0\n",
      "3  NaN   NaN\n",
      "4  5.0  10.0\n",
      " listwise deletion\n",
      "      A     B\n",
      "0  1.0   6.0\n",
      "2  3.0   8.0\n",
      "4  5.0  10.0\n",
      " pairwise deletion\n",
      "      A     B\n",
      "0  1.0   6.0\n",
      "2  3.0   8.0\n",
      "4  5.0  10.0\n"
     ]
    }
   ],
   "source": [
    "## Deletion: In this technique, we simply delete the missing data from the dataset. \n",
    "## This technique can be further divided into two categories: listwise deletion and pairwise deletion.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, 3, None, 5], 'B': [6, None, 8, None, 10]})\n",
    "\n",
    "# listwise or row wise deletion\n",
    "df1 = df.dropna()\n",
    "\n",
    "# pairwise or column wise deletion\n",
    "df2 = df[['A', 'B']].dropna()\n",
    "print(f\"before deletion\\n {df}\\n listwise deletion\\n {df1}\\n pairwise deletion\\n {df2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1c0a4d4d-075b-4c41-a3a9-c99a41ec5539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe\n",
      "      A     B\n",
      "0  1.0   6.0\n",
      "1  2.0   NaN\n",
      "2  3.0   8.0\n",
      "3  NaN   NaN\n",
      "4  5.0  10.0\n",
      " after mean imputaion\n",
      "      A     B  A_mean_imputaion  B_mean_imputaion\n",
      "0  1.0   6.0              1.00               6.0\n",
      "1  2.0   NaN              2.00               8.0\n",
      "2  3.0   8.0              3.00               8.0\n",
      "3  NaN   NaN              2.75               8.0\n",
      "4  5.0  10.0              5.00              10.0\n",
      " after median imputation\n",
      "      A     B  A_median_imputaion  B_median_imputaion\n",
      "0  1.0   6.0                 1.0                 6.0\n",
      "1  2.0   NaN                 2.0                 8.0\n",
      "2  3.0   8.0                 3.0                 8.0\n",
      "3  NaN   NaN                 2.5                 8.0\n",
      "4  5.0  10.0                 5.0                10.0\n"
     ]
    }
   ],
   "source": [
    "## Mean/median imputation: In this technique, we replace the missing data with the mean or median value of the respective column.\n",
    "\n",
    "df3 = df.copy(deep=True)\n",
    "df4 = df.copy(deep=True)\n",
    "\n",
    "# mean imputation\n",
    "df3[['A_mean_imputaion', 'B_mean_imputaion']] = df[['A', 'B']].fillna(df[['A', 'B']].mean())\n",
    "\n",
    "# median imputation\n",
    "df4[['A_median_imputaion', 'B_median_imputaion']] = df[['A', 'B']].fillna(df[['A', 'B']].median())\n",
    "\n",
    "print(f\"dataframe\\n {df}\\n after mean imputaion\\n {df3}\\n after median imputation\\n {df4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c15041b9-7379-4255-9cf4-81ee0a4448fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before mode imputaion\n",
      "      A    B\n",
      "0    a    x\n",
      "1    b  NaN\n",
      "2  NaN    y\n",
      "3    a    x\n",
      "4    a    z\n",
      " after mode imputation\n",
      "    A  B\n",
      "0  a  x\n",
      "1  b  x\n",
      "2  a  y\n",
      "3  a  x\n",
      "4  a  z\n"
     ]
    }
   ],
   "source": [
    "## Mode imputation : This technique is commonly used for categorical data or discrete numerical data where the mean or median values may not be appropriate.\n",
    "## It is a simple and quick method that can help to retain the general distribution of the data.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': ['a', 'b', np.nan, 'a', 'a'], 'B': ['x', np.nan, 'y', 'x', 'z']})\n",
    "\n",
    "df1 = df.copy(deep=True)\n",
    "\n",
    "# replace missing values with mode\n",
    "df1['A'].fillna(df1['A'].mode()[0], inplace=True)\n",
    "df1['B'].fillna(df1['B'].mode()[0], inplace=True)\n",
    "\n",
    "# print the updated dataframe\n",
    "print(f\"before mode imputaion\\n {df}\\n after mode imputation\\n {df1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f07115-0264-4490-9d63-80da2a7307b2",
   "metadata": {},
   "source": [
    "### Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "    \n",
    "        Imbalanced data refers to a situation in which the distribution of classes in a classification problem is unequal, where one class has significantly more or fewer instances than the other class(es). \n",
    "    \n",
    "        For example, in a medical diagnosis problem, the number of healthy patients may be much higher than the number of patients with a specific disease.\n",
    "        \n",
    "        It is essential to handle imbalanced data to improve the model's performance and prevent bias towards the majority class. Various techniques can be used to handle imbalanced data, such as oversampling, undersampling, generating synthetic samples, and adjusting the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb76ecb-4f21-441d-adc5-8c2447a4007d",
   "metadata": {},
   "source": [
    "### Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required.\n",
    "    \n",
    "    Down-sampling involves randomly removing instances from the majority class to balance the dataset. \n",
    "    \n",
    "        For example, if we have a dataset with 100 instances and 90 of them belong to the majority class and 10 to the minority class, we can down-sample the majority class to create a balanced dataset by randomly selecting 10 instances from the majority class.\n",
    "\n",
    "    Up-sampling, on the other hand, involves adding more instances to the minority class to balance the dataset. \n",
    "        \n",
    "        For example, if we have a dataset with 100 instances and 90 of them belong to the majority class and 10 to the minority class, we can up-sample the minority class by generating synthetic samples or replicating the existing samples to create a balanced dataset.\n",
    "        \n",
    "        For example, consider a credit card fraud detection problem where the dataset contains 10,000 instances, of which only 100 are fraudulent transactions. In this case, we have an imbalanced dataset, and the minority class is the fraudulent transactions. Since the dataset is not too large, we can up-sample the minority class to generate synthetic samples and create a balanced dataset. On the other hand, if the majority class contained redundant instances, we could down-sample it by randomly removing some instances to create a balanced dataset.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db80cc7-69be-425a-9b2e-9ba0b59ede0e",
   "metadata": {},
   "source": [
    "### Q5: What is data Augmentation? Explain SMOTE.\n",
    "\n",
    "        Data augmentation is a technique used to increase the size and diversity of a dataset by generating new samples from existing ones through various transformations such as rotation, scaling, flipping, and adding noise. The idea behind data augmentation is to improve the model's generalization ability by exposing it to more variations in the data.\n",
    "        \n",
    "        SMOTE (Synthetic Minority Over-sampling Technique) is a specific data augmentation technique used to handle imbalanced data by generating synthetic samples for the minority class. SMOTE works by synthesizing new samples in between the existing minority class samples.\n",
    "\n",
    "        The SMOTE algorithm works as follows:\n",
    "\n",
    "            For each sample in the minority class, SMOTE selects k nearest neighbors from the minority class.\n",
    "\n",
    "            SMOTE then generates new samples by interpolating between the selected minority class samples and their k nearest neighbors. To generate a new sample, SMOTE selects one of the k nearest neighbors randomly and adds a fraction of the difference between the selected sample and the neighbor to the selected sample.\n",
    "\n",
    "            The process is repeated until the desired number of synthetic samples is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb81379e-b039-4f4b-a4a3-1bbde08c4813",
   "metadata": {},
   "source": [
    "### Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "    \n",
    "        Outliers are data points that are significantly different from other data points in a dataset. Outliers can occur due to measurement errors, data entry errors, or because they represent actual extreme values in the data distribution. Outliers can affect the results of statistical analyses and machine learning models by skewing the data distribution and reducing the accuracy of the models.\n",
    "\n",
    "        It is essential to handle outliers for the following reasons:\n",
    "\n",
    "            Outliers can affect the mean and standard deviation of the data distribution, which are commonly used statistical measures. As a result, these measures may not accurately represent the central tendency and variability of the data.\n",
    "\n",
    "            Outliers can affect the performance of machine learning models. For example, linear regression models are sensitive to outliers, and outliers can cause the model to fit poorly to the data.\n",
    "\n",
    "            Outliers can also affect data visualization. For example, a box plot can be skewed by outliers, making it difficult to interpret the data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d58f281-cf59-4e19-a906-933a1a5d2b3c",
   "metadata": {},
   "source": [
    "### Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "    \n",
    "    There are several techniques that can be used to handle missing data in customer data analysis. Some of the common techniques are:\n",
    "\n",
    "        Deleting the missing data: If the amount of missing data is small, it may be appropriate to delete the missing data points from the analysis. However, this technique can result in a loss of information and can also introduce bias if the missing data is not random.\n",
    "\n",
    "        Imputation techniques: Imputation techniques involve filling in the missing data points with estimates based on the available data. The imputation technique used depends on the nature of the missing data and the type of variables involved. Some commonly used imputation techniques are:\n",
    "\n",
    "            a. Mean/Median imputation: This involves replacing the missing data with the mean / median value of the available data for that variable.\n",
    "\n",
    "            b. Mode imputation: This involves replacing the missing data with the most frequently occurring value of the available data for that variable.\n",
    "\n",
    "            c. Regression imputation: This involves using a regression model to predict the missing data based on the available data for that variable and other relevant variables.\n",
    "\n",
    "            d. K-nearest neighbor imputation: This involves using the values of the k-nearest available data points to estimate the missing data.\n",
    "\n",
    "        Multiple imputation: This involves generating multiple imputed datasets, each with different imputed values for the missing data, and combining the results from these datasets to obtain a final estimate.\n",
    "\n",
    "        Using models that can handle missing data: Some machine learning models such as Random Forest, Gradient Boosting, and Deep Learning can handle missing data by imputing the missing values internally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbf6a20-8aef-4876-bf84-103b82830a3c",
   "metadata": {},
   "source": [
    "### Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\n",
    "\n",
    "        Here are some strategies you can use to deal with missing data in a large dataset:\n",
    "\n",
    "            Identify the missing data: The first step is to identify which variables have missing data and how much data is missing. This will help you determine the best approach to deal with the missing data.\n",
    "\n",
    "            Imputation: One approach to dealing with missing data is to use imputation, which involves estimating missing values based on the available data. There are different methods of imputation, such as mean imputation, median imputation, and regression imputation, among others.\n",
    "\n",
    "            Deletion: Another approach to dealing with missing data is to simply delete the rows or columns that contain missing values. This is typically done when the percentage of missing data is very small and is unlikely to significantly impact the results.\n",
    "\n",
    "            Multiple imputation: Multiple imputation is a more advanced approach to imputation that involves creating multiple imputed datasets and analyzing each dataset separately. The results are then combined to obtain a final result that takes into account the uncertainty associated with the missing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fcd42c-8354-46a6-b75e-f3f7fdd39618",
   "metadata": {},
   "source": [
    "### Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "    \n",
    "        Here are some strategies that you can use to evaluate the performance of your machine learning model on an imbalanced dataset:\n",
    "\n",
    "        Use appropriate evaluation metrics: Accuracy is not always the best metric to evaluate performance on imbalanced datasets since it may be heavily influenced by the dominant class. Instead, you can use metrics such as precision, recall, F1-score, and area under the receiver operating characteristic (ROC) curve to evaluate the performance of your model. These metrics take into account both false positives and false negatives and are more informative for imbalanced datasets.\n",
    "\n",
    "        Use resampling techniques: Resampling techniques such as oversampling the minority class, undersampling the majority class, and generating synthetic data using techniques like SMOTE (Synthetic Minority Over-sampling Technique) can help balance the dataset and improve the performance of the machine learning model on the minority class. However, it is important to avoid overfitting and ensure that the resampling is done appropriately.\n",
    "\n",
    "        Use appropriate algorithms: Some machine learning algorithms are better suited for imbalanced datasets than others. For example, algorithms such as decision trees, random forests, and support vector machines (SVMs) can handle imbalanced datasets well. However, it is important to evaluate the performance of the algorithm on the imbalanced dataset and choose the best algorithm for the specific problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9debe5b7-f7aa-4eac-b85f-e4084c6df83b",
   "metadata": {},
   "source": [
    "### Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n",
    "\n",
    "     Here are some common techniques:\n",
    "\n",
    "        Undersampling: This involves randomly selecting a subset of the majority class to balance the dataset. One drawback of this approach is that it may discard potentially useful information that is present in the majority class.\n",
    "\n",
    "        Oversampling: This involves randomly duplicating or generating new examples of the minority class to balance the dataset. One potential issue with oversampling is that it may lead to overfitting, particularly when generating synthetic data.\n",
    "\n",
    "        Synthetic Minority Over-sampling Technique (SMOTE): This is a popular technique for generating synthetic data in the minority class to balance the dataset. SMOTE works by creating new synthetic examples by interpolating between existing examples in the minority class. This can help reduce the risk of overfitting that is associated with oversampling.\n",
    "\n",
    "        Weighted Sampling: This involves assigning weights to each example in the dataset such that the minority class examples have higher weights than the majority class examples. This can be achieved by using the inverse class frequency or other methods to calculate weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f980ae9-d491-4bca-8aaa-18f58a0c62a3",
   "metadata": {},
   "source": [
    "### Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?\n",
    "\n",
    "    Here are some common techniques:\n",
    "\n",
    "        Oversampling: This involves randomly duplicating or generating new examples of the minority class to balance the dataset. Synthetic data generation techniques such as SMOTE (Synthetic Minority Over-sampling Technique) can also be used to generate new examples of the minority class. One potential issue with oversampling is that it may lead to overfitting, particularly when generating synthetic data.\n",
    "\n",
    "        Weighted Sampling: This involves assigning weights to each example in the dataset such that the minority class examples have higher weights than the majority class examples. This can be achieved by using the inverse class frequency or other methods to calculate weights.\n",
    "        \n",
    "        Ensemble Techniques: Ensemble techniques such as bagging and boosting can combine multiple models trained on different subsets of the dataset to improve the performance of the machine learning model on the minority class. This can help balance the dataset without discarding any data or generating synthetic data.\n",
    "\n",
    "        Anomaly Detection Techniques: Anomaly detection techniques can be used to identify and extract examples of the minority class. These examples can then be used to create a balanced dataset. This approach is often used when the minority class has distinctive patterns that are different from the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aa99dc-3fb2-450a-8358-5345a7024e04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
